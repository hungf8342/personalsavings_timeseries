---
title: "Personal Savings Analysis"
author: "Andrew Brown, Melissa Hooke, Frances Hung, Mai Nguyen, Brenner Ryan"
date: "12/19/2018"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

Abstract: \textit{This paper explores personal savings data in the United States from 1955 - 1980; a span of 26 years. First, we examined the trend of our data by using various time series analysis tools such as model specification and model fitting. Through these in-depth analyses, we found three candidate models: $AR(1)$, $AR(1) \times AR(1)_4$, and $ARIMA(0,1,0) \times ARIMA(1,0,1)_6$. Further examination and diagnostic analysis of the three models, we concluded that an $AR(1) \times AR(1)_4$ was the most appropriate model for the data set. With this finding, we were able to create a forecast of the U.S. personal savings rate. These analyses allowed us to form a greater understanding of personal savings data, gain insights into the U.S.’s economic performance and perform predictions on future savings rates.}

\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(TSA)
require(dplyr)
require(astsa)
require(stats)
library(knitr)
library(ggplot2)
library(ggthemes)
```

# Introduction
The personal savings rate of consumers is one of many indicators of how a country’s economy is performing. For example, savings drive long-run economic growth as they provide funds for investment in capital or projects, which then drives future economic growth. Typically, household savings are invested either directly (i.e. when purchasing equity) or indirectly, (i.e. putting them into a bank, which uses those funds for lending) (Carroll and Mowry). These investments lead to economic growth in free-market economies. Additionally, the Harod-Domar Model of economic growth suggests that economic growth rates are driven by the level of savings. The model reasons that increased savings will lead to increased investment which result in higher capital stock, and thus, higher economic growth. An increase in savings rate would furthermore enable higher investment without a rise in net foreign borrowing. This allows a country to be less dependent on foreign capital and more insulated from possible international capital shocks that can lead to financial panic (Staff). 

\vspace{.5in}

\centerline{\includegraphics[width=10cm]{hd.jpg}}

\vspace{.5in}

Maintaining a high savings rate is also crucial for personal prosperity and financial security in retirement. The Bureau of Economics Analysis found that households aged 40 and above, those in the bottom income quartile need to save about 21 percent more on average of their pre-tax income to ensure financial security in retirement (Staff). Failure to do so would result in households either working beyond official retirement age, accepting a lower standard of living in old age, or, in the worst case, running out of money altogether (Staff). 

To understand the factors that influence personal savings rate, it is helpful to know how the savings rate is calculated. Personal savings can be understood as one minus the ratio of personal outlays (spending) to disposable income (personal income minus personal taxes). This calculation is expressed in the formula below. 

$$\text{Personal Savings Rate (\%)}=100 \times (1-\frac{\text{Personal Outlays}}{\text{Personal Income - Personal Taxes}})$$

Moreover, this shows that an increase in the personal savings rate can be associated with one of the following factors: increase in personal income, decrease in personal outlays, or decrease in personal taxes.( Carroll and Mowry)

The personal savings rate typically decreases when individuals spend more than they initially save, be it due to inflation, consumer habits, or reliance on other financial assets. Because the personal savings rate can only go so low, a low rate can be a sign of a looming recession. In contrast, after financial crises, consumers are often more cautious and tend to raise the personal savings rate. For example, in 2008, personal savings rate rose from 1.4 percent to 2.6 percent and in 2009, it reached 4.3 percent, highest since 1998. (Carroll and Mowry) This increase can be explained by the 2008 recession, after which consumers became more cautious with their spending.  

Given that saving rates are an indicator of a country’s economic performance, it is important to explore personal saving rates data in order to gain insights on the performance of economy. In this project, we are most interested in exploring the U.S. personal savings rate. By analyzing past saving rates, we are able to form a time series which forecasts future rates. This understanding will enable us to take observe the projection of the personal savings rate and take precautionary measures to ensure economic growth or brace for recessions. 


# Time Series Exploration

The original time series, as pulled from DataMarket (https://datamarket.com/), spans 26 years of personal savings as percent of disposible income in the United States. Each year of the time series is divided into financial quarters, amounting to 104 total observations, which we plot in the time series below:

```{r, echo=FALSE, fig.height=4}
# load the time series
savings = read.csv("savings.csv",header=TRUE, nrows=104)
savings = savings %>% select(2)
savings.entire = savings[1:84,]
savings<-ts(savings, start=c(1955),frequency=4)

# plot the original time series
plot(savings, xlab="Year (by Quarter)", 
     ylab= "% of Disposable Income", 
     main= "Time Series of Personal Savings in the US (1955-1980)")
points(y=savings,x=as.vector(time(savings)),pch=as.vector(season(savings)), cex=.75)
```

From our original time series plot, we see that there is a general upward trend with some sudden volatility in the 1970s that may be linked to the economic crash in the early 70s and oil energy crisis in 1979. Since the 1970s were marked by high inflation and growing expenses due to rising interest rates, we made the decision to remove the last 5 years of the time sereies since they would likely follow a different time series trend than the rest of the data.

In addition, we set aside the last 6 observations in order to use them as test points to compare with our forecasts at the end of our analysis. The resulting series of 78 observations is plotted below:

```{r, echo=FALSE,, fig.height=4}
# set aside points to validate our forecasts
savings.test = savings[79:84,]

# remove the last 5 years because of the huge dip due to recession
savings = savings[1:78,]
savings<-ts(savings, start=c(1955),frequency=4)

# plot the shortened time series with labels for quarters
plot(savings, xlab="Year (by Quarter)", 
     ylab= "% of Disposable Income", 
     main= "Time Series of Personal Savings in the US (1955-1974)")
points(y=savings,x=as.vector(time(savings)),pch=as.vector(season(savings)), cex=.75)
abline(lm(savings~time(savings)), col='blue')
```

In the time series, we see a general upward trend in the data, which indicates that the time series may not be stationary and we may want to consider taking the first difference of the data. Also, while we do not see any \textit{obvious} seasonal trends, given that the data is divided into financial quarters we may want to consider the possibility of taking a seasonal difference to make our time series stationary. First, however, let's explore without taking the difference.

The first step in analyzing our time series is to consider the possible need for a transformation to stabilize the variance of the series over time. In order to do this, we use the function \textsc{BoxCox.ar} to determine the appropriate power transformation for time-series data.


```{r,warning=FALSE, fig.height=3.5, echo=FALSE}
# should we do a transformation?
boxcox = BoxCox.ar(savings)
title("Box Cox Plot", outer = FALSE)
```

The Boxcox output indicates that a transformation is not necessary in order to stabilize the variance since $\lambda$ is about equal to 1. Therefore, we proceed by examining the acf and pacf of the series.


```{r, echo=FALSE, fig.height=3.5}
# plot the acf and pacf of the original series
acf(savings, lag.max = 25, main="ACF of Savings Time Series")
pacf(savings, lag.max = 25, main="PACF of Savings Time Series")
```

The ACF starts out large and then slowly decreases until it ends up within the white noise bounds. This kind of behavior is usually seen in ARMA and AR models. Looking at the PACF could give us more information about the nature of our data. The PACF seems to indicate that an AR(1) process may be a good candidate model because the only non-zero sample partial autocorrelation is at lag $k=1$. For lags $k \geq 1$, the partial autocorrelations appear to reduce to white-noise.

Next, we consider the differenced time series, which is plotted below:

```{r, fig.height=4, echo=FALSE}
# calculate the differenced time series
diffs = (savings-zlag(savings))[2:78]

# plot the differenced time series
plot(diffs, xlab="Year (by Quarter)", 
     ylab= "% of Disposible Income", 
     main= "Differenced Time Series of Personal Savings in the US (1955-1974)",
     type="o", cex.main=.9)
abline(lm(diffs~time(diffs)), col="blue")
```

The plot of the first difference of our time series indicates that the upward trend in the data has been removed and the mean of the differenced series is about equal to zero.

```{r, fig.height=3.5, echo=FALSE}
# plot the acf and pacf of the differenced time series
acf(diffs, lag.max = 25, main="ACF of Differenced Savings Time Series")
pacf(diffs, lag.max = 25, main= "PACF of Differenced Savings Time Series")
```

The ACF and PACF of the differenced series appear to suggest a seasonal trend in the differenced series; however, the period of this trend is unclear.

\newpage

# Model Specification

Based on the preliminary explorations of our original time series and the differenced series, the candidate models we have in mind are an AR(1) process or some type of seasonal model based on either the original or differenced series. We suspect that a seasonal model for the original series would have a seasonality of 4, but we are not sure if this seasonality will be retained in the differenced series.


In order to validate these candidate models and determine the period of possible seasonal trends, we turn to the EACF and the best subsets methods.


```{r, echo=FALSE, fig.height=3, fig.width=4,fig.show='hold',fig.align='center', eval=FALSE}
# use the eacf and best subsets to find a candidate model
eacf(savings)
sub = armasubsets(y=savings,nar=7,nma=7, y.name='test', ar.method='ols') 
plot(sub)
```
\includegraphics{original.png}

While the EACF for the original (non-differenced) time series is inconclusive, this is not a concern.  EACF is often inconclusive and not the best way of determining a useful model. We now look at the best subsets method, which indicates that an AR(1) process or a multiplicative $AR(1) \times AR(1)_4$ with a seasonal period of 4 are good candidate models.


```{r, fig.height=3, fig.width= 4, echo=FALSE, eval=FALSE}
# plot the eacf and best subsets for the differenced series
eacf(diffs)
sub = armasubsets(y=diffs,nar=7,nma=7, y.name='test', ar.method='ols') 
plot(sub)
```

\includegraphics{differenced.png}

The EACF for the differenced series also seemed to be inconclusive, and the best subsets method indicated a trend with a seasonal period of 6.  This is curious, as seasonality should theoretically be retained across a first difference.  There are a couple factors we think could be contributing to this change in seasonal period.  One factor is that a seasonal model with a period of 4 does not perfectly capture our original data. If our original data is far off enough from a true seasonal 4 model, then the first difference could have an entirely different seasonality.  The second factor is that our model includes an autoregressive component, which could also affect the first differencing process.

\newpage

# Model Fitting 

Given the results in the previous section, we have decided to fit and compare 3 different models: an AR(1), a multiplicative $AR(1) \times AR(1)_4$, and an $ARIMA(0,1,0) \times ARIMA(1,0,1)_6$ model. The parameters for each model are given in the table below:

```{r, echo = FALSE, include=FALSE}
# fit an AR(1) process
AR1model = arima(savings, order = c(1, 0, 0), seasonal = list(order = c(0, 0, 0)), method=c('ML'))
AR1model
```

```{r, echo = FALSE, include=FALSE}
# fit the seasonal model
SAR4model = arima(savings, order = c(1, 0, 0), seasonal = list(order = c(1, 0, 0), period = 4), method=c('ML'))
SAR4model
```

```{r, echo = FALSE, include=FALSE}
SAR6model = arima(savings, order = c(0, 1, 0), seasonal = list(order = c(1, 0, 1), period = 6), method=c('ML'))
SAR6model
```

```{r,echo=FALSE}
table = rbind(c("AR(1)",'6.28','0.35','.83','0.07', 'x','x','x','x',.336,-68.71,141.43),
          c("Seasonal 4",'6.27','0.35','0.86','0.06','-0.28','0.12','x','x',.3198,-66.87,139.75),
         c("Seasonal 6",'x','x','x','x','-0.18','0.38','-0.08','0.38',.3402,-67.96,139.92))
colnames(table) = c('Model','Intercept', 'se', 'ar1', 'se', 'sar1', 'se', 'sma1', 'se', 'sigma^2','log likelihood', 'aic')
kable(table)
```

Thus the equations of our 3 models are:

\begin{enumerate}
\item AR(1): $Y_t-6.28 = .83(Y_{t-1}-6.28) + e_t$
\item $AR(1) \times AR(1)_4$: $(Y_t-6.27)(1-.86(B-6.27))(1+.28(B-6.27)^4) = e_t$
\item $ARIMA(0,1,0) \times ARIMA(1,0,1)_6$: $Y_t(1-B)(1 + .18B^6) = e_t(1 + .08B^6)$
\end{enumerate}


# Diagnostics
Given the output from fitting the three models, we see that different error criterion point us to different model selections. While the AR(1) model has the lowest BIC, the seasonal model with a period of 4 has a lower standard error and AIC. Meanwhile the differenced seasonal model with a period of 6 has a similar AIC to the seasonal 4 model, but has the highest standard error. Thus, we turn to residual analysis to see if any of our models show abnormalities. 

In our residual analysis we are looking for 3 things: residual nonnormality, residual dependence, and nonconstant variance among the residuals. The presence of any of these 3 things may indicate that our model has not sufficiently identified the structure of the data and is not an adequate model. In order to do so, first we look ar the standardized residuals, the ACF of the residuals, and the Ljung-Box test for each of our three models. A plot of the standardized residuals can illustrate residual nonnormality in the model, the residual ACF looks for dependence among the residuals, and the Ljung-Box test checks if the specified model is appropriate for the data or not. For the Ljung-Box test, the alternative hypothesis is that the specified model is not appropriate for the data, so smaller p-values indicate a possibly inappropriate model.

```{r, echo = FALSE, fig.height=7}
# run some general diagnostics on the models
par(oma=c(0,0,2,0))
tsdiag(AR1model)
title("AR(1)", outer = TRUE)
```
The residuals for the AR(1) model do not indicate any nonnormality or heteroskedasticity, but the ACF shows that some lags do approach the error bounds, and the Ljung-Box statistic is borderline significant for higher lags. This indicates possible dependence among residuals.


```{r,echo=FALSE, , fig.height=7}
par(oma=c(0,0,2,0))
tsdiag(SAR4model)
title("AR(1) and Seasonal AR(1) with Period 4", outer = TRUE)
```
The residuals for the $AR(1)xAR(1)_4$ model also do not indicate any nonnormality or heteroskedasticity, all bounds of the residual ACF are within the error bounds, pointing to a lack of correlation between residuals, and the Ljung-Box statistic is not significant for all tested lags.


```{r, echo=FALSE, fig.height=7}
par(oma=c(0,0,2,0))
tsdiag(SAR6model)
title("ARIMA(0,1,0) and Seasonal ARIMA(1,0,1) with Period 6", outer = TRUE)
```
The residuals for the $ARIMA(0,1,0) \times ARIMA(1,0,1)_6$ model again do not indicate any nonnormality or heteroskedasticity, all bounds of the residual ACF are also within the error bounds, pointing to a lack of correlation between residuals, and the Ljung-Box statistic is not significant for all tested lags.

Now, we will look at the Q-Q plots and Shapiro-Wilks tests for each of our three candidate models. Q-Q plots graph the quantiles of two probability distributions on each axis - in our case, a normal distribution on the x-axis, and the distribution of our residuals on the y-axis. If the residuals are normally distributed, the plot should form a straight line along the diagonal. The farther the plot strays from the diagonal, the less normal our residuals are. Similarly, the Shapiro-Wilks test tests for normality among standardized residuals. The alternative hypothesis of the Shapiro-Wilks test is that the standardized residuals of a certain model are not normally distributed, so smaller significant p-values indicate nonnormality among residuals.

```{r, echo = FALSE, fig.height=3.5, fig.width=3.5}
ARresids = rstandard(AR1model)
SAR4resids = rstandard(SAR4model)
SAR6resids = rstandard(SAR6model)

# plot the residuals for normality
qqnorm(ARresids,main="AR(1) Residuals",
       cex.lab=.85, cex.axis=.85, cex.main=.85, cex.sub=.85); qqline(ARresids)
qqnorm(SAR4resids,main="Seasonal 4 Residuals",
       cex.lab=.85, cex.axis=.85, cex.main=.85, cex.sub=.85); qqline(SAR4resids)
```

```{r, echo = FALSE, fig.height=3.5, fig.width=3.5}
qqnorm(SAR6resids,main="Differenced Seasonal 6 Residuals",
       cex.lab=.85, cex.axis=.85, cex.main=.85, cex.sub=.85); qqline(SAR4resids)
```


\newpage 

#Shapiro-Wilks Test
```{r, echo=FALSE}
table = cbind(c('W','p-value'),c(.981,.295),c(.983,.369),c(.979,.237))
colnames(table)=c('Model','AR(1)','Seasonal 4','Seasonal 6')
kable(table)
```

```{r, echo = FALSE,eval=FALSE}
# test the residuals for normality
shapiro.test(ARresids)
shapiro.test(SAR4resids)
shapiro.test(SAR6resids)
```

Based on their Q-Q plots and their Shapiro-Wilk tests, none of the models show evidence of residual nonnormality.  

Finally, we will turn to runs tests. A run is a sequence of increasing or decreasing values, so a large difference in expected vs. observed runs may point towards residual dependence. Runs tests test for independence among standardized residuals of a certain model. The alternative hypothesis of the runs test is that the standardized residuals of a certain model are not independent, so smaller p-values indicate dependence among residuals.


#Runs Test
```{r, eval = FALSE, echo=FALSE}
# test the residuals for independence
runs(ARresids)
runs(SAR4resids)
runs(SAR6resids)
```

```{r,echo=FALSE}
table = cbind(c("p-value","Observed runs","Expected runs", "n1","n2","k"),
              c(0.73, 42, 30, 38, 40, 0),
              c(0.37,44,40,35,43,0),
              c(0.18,45,39,32,46,0))
colnames(table) = c("Model","AR(1)","Seasonal 4","Seasonal 6")
kable(table)
```


Our runs tests support the Ljung-Box tests, which indicated that there is no sufficient evidence to reject residual independence.

\newpage 

# Forecasting

In our previous section, we concluded that the best model for our series is an $ARIMA(0,1,0) \times ARIMA(1,0,1)_6$. We now want to predict the values of the series at future times. This forecast will enable us to gain insights into the direction of which the savings rate is headed as well as the future health of the economy. 

We forecasted our model with a built-in forecast ARIMA function in R. The red line signifies the forecasted values with a confidence bound of one standard error in darker grey and of two standard errors in lighter grey. We observed that the forecasted values increase at first and then decay back to the upward linear trend. Additionally, the forecasted values generally maintain the previous seasonal pattern. 

Comparing the actual values of the time series which we set aside as test points, the next 6 points in the time series remain within one standard deviations of our forecasted values, which shows that our forecasts are a good prediction of our data. However, after that point since we cut off the last 5 years of our data due to economic volatility, it is likely that the predictions would not be quite as accurate, as with predicting far into the future for any time series.


```{r,include=FALSE, echo=FALSE}
forecasts = sarima.for(savings,n.ahead = 10, 0, 1, 0, P = 1, D = 0, Q = 1, S = 6)
Standard_error=c(rep(0,78),forecasts$se)
preds = forecasts$pred
actuals = c(rep(NA,78),savings.test)
is.pred = as.factor(c(rep("No",78),rep("Yes",10)))
savings.df = as.data.frame(cbind(seq(1955,1976.75,by=.25),c(savings, preds),is.pred, actuals))
names(savings.df) = c("Time", "Savings","ispred", "actuals")
``` 

```{r, echo=FALSE, warning=FALSE}
ggplot(savings.df, aes(x = Time, y = Savings)) +
       geom_point(aes(color=is.pred)) + geom_line(aes(color=is.pred)) +
       geom_ribbon(aes(ymin = Savings - Standard_error,
                       ymax = Savings + Standard_error), alpha = 0.2) +
  geom_ribbon(aes(ymin = Savings - 2*Standard_error,
                       ymax = Savings + 2*Standard_error), alpha = 0.2) +
  geom_point(aes(x=Time,y=actuals)) +theme_classic() + theme(legend.position="none") +
  scale_color_manual(values=c("black", "red", "blue")) + 
  xlab("Year (by Quarter)") + 
  ylab("% of Disposable Income") +
  ggtitle("Time Series of Personal Savings with Forecasts (1955-1975)")
  
```


\newpage

# Discussion

The goal of the present analysis on the personal savings data was to construct a time series model which would adequately capture the savings trends across the data and accurately predict future data points in the years to come. In our analysis, we considered 3 different ARIMA and SARIMA models: a basic AR(1) process, a seasonal model with a period of 4, and a differenced seasonal model with a period of 6. Through our model diagnostics, we settled on the differenced seasonal 6 model as most appropriate model because removing the trend from the time series appeared to sufficiently reduce the series to a white noise process with normal and independent residuals. 

Overall, our analysis concludes that the savings rate will decrease in the near future time points as predicted by the model. However, our analysis also supports a general upward trend of the savings rate. This suggests that the economy is heading towards a temporary cyclical trend of choosing short-term consumption over long-term investment, but generally, savings rates are growing. A continuation of this temporary downward trend can indicate a looming economic stagnation. We hope that our analysis will help indicate when the savings rate becomes abnormally low or high compared to our forecasts, which may be a sign of impending economic trouble.

One major challenge that we experienced in choosing our final model was the debate between the seasonal 4 model and the differenced seasonal 6 model. The fact that the data was broken down by financial quarter made the appearance of the seasonal 4 trend enticing. However, we decided that the upward trend in the data was too strong to ignore in the final model of the time series. Differencing the time series as we did in the final chosen model adequately preserved the upward trend in personal savings over time. That said, the upward trend in savings cannot increase forever since the savings are expressed as a \textit{percentage} of disposable income. If this rate were to continue indefinitely, that would eventually lead to everyone saving their money and never spending, which would lead to an unhealthy economy. Thus, it makes sense that the upward trend in our data would eventually break down.

We acknowledge that our analysis has a few shortcomings. The first one is that we cut off the last 5 years of our data set due to economic volatility. As a consequence of this, our model does not include nor predict those 5 years of drastically tumbling savings rates. Moreover, our analysis is not generalizable to today’s savings rate because this data was captured 40 years ago. Though our time series provided insights into the past health of the economy, we would also be interested in seeing a similar time series of more current data. If the same seasonal and upward trend is prevalent for today’s data, it would be supportive of economic commonalities which drive savings rate changes back from the 70’s through today. Repetition of the same trend may indicate that our savings economy follows some upward trend until an economic crash, then resets and begins the pattern over again. Of course there are so many variables that go into our decisions to save more or spend more as a society so it is impossible to tell what is actually driving the upward trends and interruptions in that trend.
 
\newpage

#Bibliography

“Personal Savings as % of Disposable Income 1955-1979.” DataMarket, https://datamarket.com/data/set/22vy/personal-savings-as-of-disposable-income-1955-1979#!ds=22vy&display=line

Sullivan, Bob. “Once Again, Americans Are Not Saving Enough.” MarketWatch, MarketWatch, 28 Aug. 2018, www.marketwatch.com/story/once-again-americans-are-not-saving-enough-2018-08-28.


Carroll, Daniel, and Beth Mowry. “Personal Savings Up, National Savings Down.” Economic Trends (07482922), vol. Apr2010, P8, 2010.


Pettinger, Tejvan. “Would an Increase in Savings Help the Economy?” Economics Help, www.economicshelp.org/blog/7102/economics/would-an-increase-in-savings-help-the-economy/


“Understanding the Economic Benefits of Increased Saving | Commentary.” Roll Call, 26 June 2014, www.rollcall.com/news/understanding_the_economic_benefits_of_increased_saving_commentary-234283-1.html.

\newpage

# Appendix: R Code

```{r, eval=FALSE, fig.height=4}
# load the time series
savings = read.csv("savings.csv",header=TRUE, nrows=104)
savings = savings %>% select(2)
savings.entire = savings[1:84,]
savings<-ts(savings, start=c(1955),frequency=4)

# plot the original time series
plot(savings, xlab="Year (by Quarter)", 
     ylab= "% of Disposible Income", 
     main= "Time Series of Personal Savings in the US (1955-1980)")
points(y=savings,x=as.vector(time(savings)),pch=as.vector(season(savings)), cex=.75)
```

```{r, eval=FALSE,, fig.height=4}
# set aside points to validate our forecasts
savings.test = savings[79:84,]

# remove the last 5 years because of the huge dip due to recession
savings = savings[1:78,]
savings<-ts(savings, start=c(1955),frequency=4)

# plot the shortened time series with labels for quarters
plot(savings, xlab="Year (by Quarter)", 
     ylab= "% of Disposible Income", 
     main= "Time Series of Personal Savings in the US (1955-1980)")
points(y=savings,x=as.vector(time(savings)),pch=as.vector(season(savings)), cex=.75)
abline(lm(savings~time(savings)), col='blue')
```

```{r,warning=FALSE, eval=FALSE}
# should we do a transformation?
boxcox = BoxCox.ar(savings)
boxcox
```

```{r, eval=FALSE, fig.height=3.5}
# plot the acf and pacf of the original series
acf(savings, lag.max = 25)
pacf(savings, lag.max = 25)
```

```{r, fig.height=4, eval=FALSE}
# calculate the differenced time series
diffs = (savings-zlag(savings))[2:78]

# plot the differenced time series
plot(diffs, xlab="Year (by Quarter)", 
     ylab= "% of Disposible Income", 
     main= "Differenced Time Series of Personal Savings in the US (1955-1980)", type="o")
abline(lm(diffs~time(diffs)), col="blue")
```

```{r, fig.height=3.5, eval=FALSE}
# plot the acf and pacf of the differenced time series
acf(diffs, lag.max = 25)
pacf(diffs, lag.max = 25)
```

```{r, eval=FALSE}
# use the eacf and best subsets to find a candidate model
eacf(savings)
sub = armasubsets(y=savings,nar=7,nma=7, y.name='test', ar.method='ols') 
plot(sub)
```

```{r, fig.height=3, eval=FALSE}
# plot the eacf and best subsets for the differenced series
eacf(diffs)
sub = armasubsets(y=diffs,nar=7,nma=7, y.name='test', ar.method='ols') 
plot(sub)
```

```{r, eval = FALSE}
# fit an AR(1) process
AR1model = arima(savings, order = c(1, 0, 0), seasonal = list(order = c(0, 0, 0)), method=c('ML'))
AR1model
```

```{r, eval = FALSE, include=FALSE}
# fit the seasonal model
SAR4model = arima(savings, order = c(1, 0, 0), seasonal = list(order = c(1, 0, 0), period = 4), method=c('ML'))
SAR4model
```

```{r, eval = FALSE, include=FALSE}
# fit the differenced seasonal model
SAR6model = arima(savings, order = c(0, 1, 0), seasonal = list(order = c(1, 0, 1), period = 6), method=c('ML'))
SAR6model
```

```{r, eval = FALSE}
# run some general diagnostics on the models
tsdiag(AR1model)
tsdiag(SAR4model)
tsdiag(SAR6model)
```

```{r, eval = FALSE}
# test the residuals for normality
shapiro.test(ARresids)
shapiro.test(SAR4resids)
shapiro.test(SAR6resids)
```

```{r, eval = FALSE}
# test the residuals for independence
runs(ARresids)
runs(SAR4resids)
runs(SAR6resids)
```

