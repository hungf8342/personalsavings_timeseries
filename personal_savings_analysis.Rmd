---
title: "Personal Savings Analysis"
author: "Andrew Brown, Melissa Hooke, Frances Hung, Mai Nguyen, Brenner Ryan"
date: "12/19/2018"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

Abstract: \textit{This paper explores personal savings data in the United States from 1955 - 1980; a span of 26 years. We first examined the trend of our data by using various time series analysis tools such as model specification, model fitting and diagnostics of our fitted models. Through these in-depth analyses, we found three candidate models: $AR(1)$, $AR(1) \times AR(1)_4$, and $ARIMA(0,1,0) \times ARIMA(1,0,1)_6$. Further examination concluded that $AR(1) \times AR(1)_4$ is the most fitting model for the data set. With this model, we are able to form a greater understanding of personal savings data, gain insights into the U.S.â€™s economic performance and perform predictions on future savings rates.}

\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(TSA)
require(dplyr)
require(astsa)
require(stats)
library(knitr)
```

# Introduction



# Time Series Exploration

The original time series, as pulled from DataMarket (https://datamarket.com/), spans 26 years of personal savings as percent of disposible income in the United States. Each year of the time series is divided into financial quarters, amounting to 104 total observations, which we plot in the time series below:

```{r, echo=FALSE, fig.height=4}
# load the time series
savings = read.csv("savings.csv",header=TRUE, nrows=104)
savings = savings %>% select(2)
savings.entire = savings[1:84,]
savings<-ts(savings, start=c(1955),frequency=4)

# plot the original time series
plot(savings, xlab="Year (by Quarter)", 
     ylab= "% of Disposible Income", 
     main= "Time Series of Personal Savings in the US (1955-1980)")
points(y=savings,x=as.vector(time(savings)),pch=as.vector(season(savings)), cex=.75)
```

From our original time series plot, we see that there is a general upward trend with some sudden volatility in the 1970s that may be linked to the economic crash in the early 70s and oil energy crisis in 1979. Since the 1970s were marked by high inflation and growing expenses due to rising interest rates, we made the decision to remove the last 5 years of the time sereies since they would likely follow a different time series trend than the rest of the data.

In addition, we set aside the last 6 observations in order to use them as test points to compare with our forecasts at the end of our analysis. The resulting series of 78 observations is plotted below:

```{r, echo=FALSE,, fig.height=4}
# set aside points to validate our forecasts
savings.test = savings[79:84,]

# remove the last 5 years because of the huge dip due to recession
savings = savings[1:78,]
savings<-ts(savings, start=c(1955),frequency=4)

# plot the shortened time series with labels for quarters
plot(savings, xlab="Year (by Quarter)", 
     ylab= "% of Disposible Income", 
     main= "Time Series of Personal Savings in the US (1955-1974)")
points(y=savings,x=as.vector(time(savings)),pch=as.vector(season(savings)), cex=.75)
abline(lm(savings~time(savings)), col='blue')
```

In the time series, we see a general upward trend in the data, which indicates that the time series may not be stationary and we may want to consider taking the first difference of the data. Also, while do not see any \textit{obvious} seasonal trends, given that the data is divided into financial quarters we may want to consider the possibility of taking a seasonal difference to make our time series stationary. First, however, let's explore without taking the difference.

The first step in analyzing our time series is to consider the possible need for a transformation to stabilize the variance of the series over time. In order to do this, we use the function \textsc{BoxCox.ar} to determine the appropriate power transformation for time-series data.


```{r,warning=FALSE, fig.height=3.5, echo=FALSE}
# should we do a transformation?
boxcox = BoxCox.ar(savings)
title("Box Cox Plot", outer = FALSE)
```

The Boxcox output indicates that a transformation is not necessary in order to stabilize the variance since $\lambda$ is about equal to 1. Therefore, we proceed by examining the acf and pacf of the series.


```{r, echo=FALSE, fig.height=3.5}
# plot the acf and pacf of the original series
acf(savings, lag.max = 25, main="ACF of Savings Time Series")
pacf(savings, lag.max = 25, main="PACF of Savings Time Series")
```

The ACF starts out large and then slowly decreases until it ends up within the white noise bounds. This kind of behavior is usually seen in ARMA and AR models. Looking at the PACF could give us more information about the nature of our data. The PACF seems to indicate that an AR(1) process may be a good candidate model because the only non-zero sample partial autocorrelation is at lag $k=1$. For lags $k \geq 1$, the partial autocorrelations appear to reduce to white-noise.

Next, we consider the differenced time series, which is plotted below:

```{r, fig.height=4, echo=FALSE}
# calculate the differenced time series
diffs = (savings-zlag(savings))[2:78]

# plot the differenced time series
plot(diffs, xlab="Year (by Quarter)", 
     ylab= "% of Disposible Income", 
     main= "Differenced Time Series of Personal Savings in the US (1955-1974)",
     type="o", cex.main=.9)
abline(lm(diffs~time(diffs)), col="blue")
```

The plot of the first difference of our time series indicates that the upward trend in the data has been removed and the mean of the differenced series is about equal to zero.

```{r, fig.height=3.5, echo=FALSE}
# plot the acf and pacf of the differenced time series
acf(diffs, lag.max = 25, main="ACF of Differenced Savings Time Series")
pacf(diffs, lag.max = 25, main= "PACF of Differenced Savings Time Series")
```

The ACF and PACF of the differenced series appear to suggest a seasonal trend in the differenced series, however, the period of this trend is unclear.

\newpage

# Model Specification

Based on the preliminary explorations of our original time series and the differenced series, the candidate models are have in mind are an AR(1) process or some type of seasonal model (period 4?) based on either the original or differenced series.

In order to validate these candidate models and determine the period of possible seasonal trends, we turn to the EACF and the best subsets methods.



```{r, echo=FALSE, fig.height=3, fig.width=4,fig.show='hold',fig.align='center'}
# use the eacf and best subsets to find a candidate model
eacf(savings)
sub = armasubsets(y=savings,nar=7,nma=7, y.name='test', ar.method='ols') 
plot(sub)
```

While the EACF for the original (non-differenced) time series is inconclusive, this is not a concern.  EACF is often inconclusive and not the best way of determining a useful model. The best subsets method indicates that, an AR(1) process or a multiplicative $AR(1) \times AR(1)_4$ with a seasonal period of 4 are good candidate models.


```{r, fig.height=3, fig.width= 4, echo=FALSE}
# plot the eacf and best subsets for the differenced series
eacf(diffs)
sub = armasubsets(y=diffs,nar=7,nma=7, y.name='test', ar.method='ols') 
plot(sub)
```

The EACF for the differenced series also seemed to be inconclusive, and the best subsets method indicated a trend with a seasonal period of 6.



# Model Fitting 

Given the results in the previous section, we have decided to fit and compare 3 different models: an AR(1), a multiplicative $AR(1) \times AR(1)_4$, and an $ARIMA(0,1,0) \times ARIMA(1,0,1)_6$ model. The parameters for each model are given in the table below:

```{r, echo = FALSE, include=FALSE}
# fit an AR(1) process
AR1model = arima(savings, order = c(1, 0, 0), seasonal = list(order = c(0, 0, 0)), method=c('ML'))
AR1model
```

```{r, echo = FALSE, include=FALSE}
# fit the seasonal model
SAR4model = arima(savings, order = c(1, 0, 0), seasonal = list(order = c(1, 0, 0), period = 4), method=c('ML'))
SAR4model
```

```{r, echo = FALSE, include=FALSE}
SAR6model = arima(savings, order = c(0, 1, 0), seasonal = list(order = c(1, 0, 1), period = 6), method=c('ML'))
SAR6model
```

```{r,echo=FALSE}
table = rbind(c("AR(1)",'6.28','0.35','.83','0.07', 'x','x','x','x',.336,-68.71,141.43),
          c("Seasonal 4",'6.27','0.35','0.86','0.06','-0.28','0.12','x','x',.3198,-66.87,139.75),
         c("Seasonal 6",'x','x','x','x','-0.18','0.38','-0.08','0.38',.3402,-67.96,139.92))
colnames(table) = c('Model','Intercept', 'se', 'ar1', 'se', 'sar1', 'se', 'sma1', 'se', 'sigma^2','log likelihood', 'aic')
kable(table)
```

Thus the equations of our 3 models are:

\begin{enumerate}
\item AR(1): $Y_t-6.28 = .825(Y_{t-1}-6.28) + e_t$
\item $AR(1) \times AR(1)_4$: $(Y_t-6.27)(1-.861(B-6.27))(1+.228(B-6.27)^4) = e_t$
\item $ARIMA(0,1,0) \times ARIMA(1,0,1)_6$: $Y_t$
\end{enumerate}




# Diagnostics
Given the output from fitting the three models, we see that different error criterion point us to different model selections. While, the AR(1) model has the lowest BIC, but the seasonal model with a period of 4 has a lower standard error and AIC. Meanwhile the differenced seasonal model with a period of 6 has a similar AIC to the seasonal 4 model, but has the highest standard error. Thus, we turn to residual analysis to see if any of our models show abnormalities. 

In our residual analysis we are looking for 3 things: residual nonnormality, residual dependence, and residual structure. The presence of any of these 3 things may indicate that our model has not sufficiently identified the structure of the data and is not an adequate model.

```{r, echo = FALSE, fig.height=7}
# run some general diagnostics on the models
par(oma=c(0,0,2,0))
tsdiag(AR1model)
title("AR(1)", outer = TRUE)
```
The residuals for the AR(1) model resemble a white-noise process, the ACF shows no correlation patterns between residuals, and the Ljung-Box statistic is borderline significant for higher lags. This indicates possible dependence among residuals.


```{r,echo=FALSE, , fig.height=7}
par(oma=c(0,0,2,0))
tsdiag(SAR4model)
title("AR(1) and Seasonal AR(1) with Period 4", outer = TRUE)
```
The residuals for the $AR(1)xAR(1)_4$ model resemble a white-noise process, the ACF shows no correlation patterns between residuals, and the Ljung-Box statistic is not significant for all tested lags.


```{r, echo=FALSE, fig.height=7}
par(oma=c(0,0,2,0))
tsdiag(SAR6model)
title("ARIMA(0,1,0) and Seasonal ARIMA(1,0,1) with Period 6", outer = TRUE)
```
The residuals for the $ARIMA(0,1,0) \times ARIMA(1,0,1)_6$ model resemble a white-noise process, the ACF shows no correlation patterns between residuals, and the Ljung-Box statistic is not significant for all tested lags.

```{r, echo = FALSE, fig.height=3.25, fig.width=3.25}
ARresids = rstandard(AR1model)
SAR4resids = rstandard(SAR4model)
SAR6resids = rstandard(SAR6model)

# plot the residuals for normality
qqnorm(ARresids,main="AR(1) residuals",
       cex.lab=.85, cex.axis=.85, cex.main=.85, cex.sub=.85); qqline(ARresids)
qqnorm(SAR4resids,main="SAR4 residuals",
       cex.lab=.85, cex.axis=.85, cex.main=.85, cex.sub=.85); qqline(SAR4resids)
qqnorm(SAR6resids,main="SAR6 residuals",
       cex.lab=.85, cex.axis=.85, cex.main=.85, cex.sub=.85); qqline(SAR4resids)
```

#Shapiro-Wilks Test
```{r, echo=FALSE}
table = cbind(c('W','p-value'),c(.981,.295),c(.983,.369),c(.979,.237))
colnames(table)=c('Model','AR(1)','Seasonal 4','Seasonal 6')
kable(table)
```

```{r, echo = FALSE,eval=FALSE}
# test the residuals for normality
shapiro.test(ARresids)
shapiro.test(SAR4resids)
shapiro.test(SAR6resids)
```

Based on their Q-Q plots and their Shapiro-Wilk tests, none of the models show evidence of residual nonnormality.  

#Runs Test
```{r, eval = FALSE, echo=FALSE}
# test the residuals for independence
runs(ARresids)
runs(SAR4resids)
runs(SAR6resids)
```

```{r,echo=FALSE}
table = cbind(c("p-value","Observed runs","Expected runs", "n1","n2","k"),
              c(0.73, 42, 30, 38, 40, 0),
              c(0.37,44,40,35,43,0),
              c(0.18,45,39,32,46,0))
colnames(table) = c("Model","AR(1)","Seasonal 4","Seasonal 6")
kable(table)
```


Our runs tests support the Ljung-Box tests, which indicated that there is no sufficient evidence to reject residual independence.


# Forecasting



```{r, fig.height=3, echo=FALSE}
sarima.for(savings,n.ahead = 6, 1, 0, 0, P = 0, D = 0, Q = 0, S = 0)
points(79:84,savings.test, col="blue")
```

```{r, fig.height=3, echo=FALSE}
sarima.for(savings,n.ahead = 6, 1, 0, 0, P = 1, D = 0, Q = 0, S = 4)
points(79:84,savings.test, col="blue")
```

```{r, fig.height=3, echo=FALSE}
sarima.for(savings,n.ahead = 6, 0, 1, 0, P = 1, D = 0, Q = 1, S = 6)
points(79:84,savings.test, col="blue")
```


# Appendix: R Code

```{r, eval=FALSE, fig.height=4}
# load the time series
savings = read.csv("savings.csv",header=TRUE, nrows=104)
savings = savings %>% select(2)
savings.entire = savings[1:84,]
savings<-ts(savings, start=c(1955),frequency=4)

# plot the original time series
plot(savings, xlab="Year (by Quarter)", 
     ylab= "% of Disposible Income", 
     main= "Time Series of Personal Savings in the US (1955-1980)")
points(y=savings,x=as.vector(time(savings)),pch=as.vector(season(savings)), cex=.75)
```

```{r, eval=FALSE,, fig.height=4}
# set aside points to validate our forecasts
savings.test = savings[79:84,]

# remove the last 5 years because of the huge dip due to recession
savings = savings[1:78,]
savings<-ts(savings, start=c(1955),frequency=4)

# plot the shortened time series with labels for quarters
plot(savings, xlab="Year (by Quarter)", 
     ylab= "% of Disposible Income", 
     main= "Time Series of Personal Savings in the US (1955-1980)")
points(y=savings,x=as.vector(time(savings)),pch=as.vector(season(savings)), cex=.75)
abline(lm(savings~time(savings)), col='blue')
```

```{r,warning=FALSE, eval=FALSE}
# should we do a transformation?
boxcox = BoxCox.ar(savings)
boxcox
boxcox$mle
```

```{r, eval=FALSE, fig.height=3.5}
# plot the acf and pacf of the original series
acf(savings, lag.max = 25)
pacf(savings, lag.max = 25)
```

```{r, fig.height=4, eval=FALSE}
# calculate the differenced time series
diffs = (savings-zlag(savings))[2:78]

# plot the differenced time series
plot(diffs, xlab="Year (by Quarter)", 
     ylab= "% of Disposible Income", 
     main= "Differenced Time Series of Personal Savings in the US (1955-1980)", type="o")
abline(lm(diffs~time(diffs)), col="blue")
```

```{r, fig.height=3.5, eval=FALSE}
# plot the acf and pacf of the differenced time series
acf(diffs, lag.max = 25)
pacf(diffs, lag.max = 25)
```

```{r, eval=FALSE}
# use the eacf and best subsets to find a candidate model
eacf(savings)
sub = armasubsets(y=savings,nar=7,nma=7, y.name='test', ar.method='ols') 
plot(sub)
```

```{r, fig.height=3, eval=FALSE}
# plot the eacf and best subsets for the differenced series
eacf(diffs)
sub = armasubsets(y=diffs,nar=7,nma=7, y.name='test', ar.method='ols') 
plot(sub)
```

```{r, eval = FALSE}
# fit an AR(1) process
AR1model = arima(savings, order = c(1, 0, 0), seasonal = list(order = c(0, 0, 0)), method=c('ML'))
AR1model
```

```{r, eval = FALSE, include=FALSE}
# fit the seasonal model
SAR4model = arima(savings, order = c(1, 0, 0), seasonal = list(order = c(1, 0, 0), period = 4), method=c('ML'))
SAR4model
```

```{r, eval = FALSE, include=FALSE}
# fit the differenced seasonal model
SAR6model = arima(savings, order = c(0, 1, 0), seasonal = list(order = c(1, 0, 1), period = 6), method=c('ML'))
SAR6model
```

```{r, eval = FALSE}
# run some general diagnostics on the models
tsdiag(AR1model)
tsdiag(SAR4model)
tsdiag(SAR6model)
```

```{r, eval = FALSE}
# test the residuals for normality
shapiro.test(ARresids)
shapiro.test(SAR4resids)
shapiro.test(SAR6resids)
```

```{r, eval = FALSE}
# test the residuals for independence
runs(ARresids)
runs(SAR4resids)
runs(SAR6resids)
```

